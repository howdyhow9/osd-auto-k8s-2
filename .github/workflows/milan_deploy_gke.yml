name: Deploy Data Platform to GKE

on:
  push:
    branches:
      - main

env:
  CLUSTER_NAME: osd-k8s-cluster
  CLUSTER_ZONE: us-east1
  USER_EMAIL: gvsbharish@gmail.com

jobs:
  setup-infrastructure:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth
        id: auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Install GKE Auth Plugin
        run: |
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
          sudo apt-get update
          sudo apt-get install -y google-cloud-sdk-gke-gcloud-auth-plugin

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      - name: Create Namespaces
        run: |
          for ns in spark-apps airflow spark-operator trino kafka dbt superset; do
            kubectl create namespace $ns || echo "Namespace $ns already exists"
          done

  deploy-spark:
    needs: setup-infrastructure
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth & kubectl setup
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      - name: Create ClusterRoleBinding
        run: kubectl apply -f spark8s/spark_svc_account.yaml -n spark-apps

      - name: Install Spark Operator
        run: |
          helm upgrade --install osds-release spark8s/spark-operator-1.1.27/spark-operator \
            -n spark-operator \
            --create-namespace \
            --wait \
            --timeout 2m

      - name: Install Spark Thrift Server
        run: kubectl apply -f spark8s/spark-thriftserver.yaml -n spark-apps

  deploy-postgres-and-hive:
    needs: setup-infrastructure
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth & kubectl setup
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Deploy PostgreSQL for Hive
        run: |
          kubectl apply -f spark8s/postgres_hive_ms/postgres_pvc_hive_ms.yaml -n spark-apps
          kubectl apply -f spark8s/postgres_hive_ms/postgres_secret_hive_ms.yaml -n spark-apps
          kubectl apply -f spark8s/postgres_hive_ms/postgres_stateful_hive_ms.yaml -n spark-apps

      - name: Deploy Hive Metastore
        run: |
          kubectl apply -f spark8s/hive-metastore/hive-metastore-cgf.yaml -n spark-apps
          kubectl apply -f spark8s/hive-metastore/hive-metastore.yaml -n spark-apps
          kubectl apply -f spark8s/hive-metastore/hive-metastore-svc.yaml -n spark-apps
          kubectl apply -f spark8s/hive-metastore/hive-schema.yaml -n spark-apps

  deploy-trino:
    needs: [deploy-postgres-and-hive]
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth & kubectl setup
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      - name: Install Trino
        run: |
          helm repo add trino https://trinodb.github.io/charts
          helm repo update
          helm upgrade --install --values trino/values.yaml trino trino/trino -n trino

  deploy-airflow:
    needs: [deploy-spark, deploy-postgres-and-hive]
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth & kubectl setup
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      - name: Deploy Airflow PostgreSQL
        run: |
          kubectl apply -f airflow_k8s/postgres_airflow/postgres_secret_airflow.yaml -n airflow
          kubectl apply -f airflow_k8s/postgres_airflow/postgres_pvc_airflow.yaml -n airflow
          kubectl apply -f airflow_k8s/postgres_airflow/postgres_stateful_airflow.yaml -n airflow

      - name: Create Airflow Secrets
        run: |
          kubectl create secret generic airflow-postgres \
            --from-literal=connection=postgresql://airflow:airflowpass@postgres-airflow:5432/airflow_metastore \
            -n airflow || echo "Secret airflow-postgres already exists"
          
          kubectl create secret generic osds-webserver-secret \
            --from-literal="webserver-secret-key=$(python3 -c 'import secrets; print(secrets.token_hex(16))')" \
            -n airflow || echo "Secret osds-webserver-secret already exists"

      - name: Deploy Airflow PVC and Install
        run: |
          kubectl apply -f airflow_k8s/airflow-pvc.yaml -n airflow
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update
          helm upgrade --install airflow apache-airflow/airflow \
            --namespace airflow \
            --values airflow_k8s/airflow-values.yaml \
            --debug \
            --timeout 10m || echo "Airflow Helm release already exists"

      - name: Create Airflow-Spark ClusterRoleBinding
        run: |
          kubectl create clusterrolebinding default-admin \
            --clusterrole cluster-admin \
            --serviceaccount=airflow:airflow-worker \
            --namespace spark-apps || echo "ClusterRoleBinding already exists"

  deploy-kafka:
    needs: setup-infrastructure
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth & kubectl setup
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      - name: Install Strimzi Kafka Operator
        run: |
          helm repo add strimzi https://strimzi.io/charts/
          helm repo update
          helm upgrade --install strimzi strimzi/strimzi-kafka-operator \
            --namespace kafka \
            --wait || echo "Strimzi Helm release already exists"

      - name: Deploy Kafka
        run: |
          kubectl apply -f kafka/kafka-persistent-single.yaml -n kafka
          kubectl apply -f kafka/kafka-svc.yaml -n kafka

  deploy-dbt-and-superset:
    needs: [deploy-postgres-and-hive, deploy-trino]
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth & kubectl setup
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      - name: Setup dbt
        run: |
          kubectl create serviceaccount dbt -n dbt || echo "ServiceAccount dbt already exists"
          kubectl create clusterrolebinding dbt-role-binding \
            --clusterrole=edit \
            --serviceaccount=dbt:dbt \
            --namespace=dbt || echo "ClusterRoleBinding dbt-role-binding already exists"

      - name: Install Superset
        run: |
          helm repo add superset https://apache.github.io/superset
          helm repo update
          helm upgrade --install --values superset/superset-values.yaml \
            superset superset/superset -n superset --wait || echo "Superset Helm release already exists"

  cleanup:
    needs: [deploy-postgres-and-hive]
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth & kubectl setup
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Delete Hive Schema Pod
        run: kubectl delete -f spark8s/hive-metastore/hive-schema.yaml -n spark-apps