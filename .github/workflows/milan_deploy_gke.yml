name: Deploy PostgreSQL to GKE

on:
  push:
    branches:
      - main  # Trigger deployment on changes to the main branch

env:
  CLUSTER_NAME: osd-k8s-cluster
  CLUSTER_ZONE: us-east1

jobs:
  # Job for GKE Setup and Namespace Creation
  setup_gke:
    runs-on: ubuntu-latest

    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Google Auth
        id: auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Install GKE Auth Plugin
        run: |
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - 
          sudo apt-get update
          sudo apt-get install -y google-cloud-sdk-gke-gcloud-auth-plugin

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Create namespaces
        run: |
          kubectl create namespace spark-apps || echo "Namespace spark-apps already exists"
          kubectl create namespace airflow || echo "Namespace airflow already exists"
          kubectl create namespace spark-operator || echo "Namespace spark-operator already exists"
          kubectl create namespace trino || echo "Namespace trino already exists"
          kubectl create namespace kafka || echo "Namespace kafka already exists"
          kubectl create namespace dbt || echo "Namespace dbt already exists"
          kubectl create namespace superset || echo "Namespace superset already exists"

  # Job for Helm Setup and Spark Operator Deployment
  helm_and_spark_operator:
    runs-on: ubuntu-latest
    needs: setup_gke  # Dependent on setup_gke job

    steps:
      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      - name: Create ClusterRoleBinding for Spark
        run: |
          kubectl apply -f spark8s/spark_svc_account.yaml -n spark-apps

      - name: Install Spark Operator
        run: |
          helm upgrade --install osds-release spark8s/spark-operator-1.1.27/spark-operator \
            -n spark-operator \
            --create-namespace \
            --wait \
            --timeout 2m

  # Job for Deploying PostgreSQL and Hive Metastore
  deploy_postgres_and_hive:
    runs-on: ubuntu-latest
    needs: helm_and_spark_operator  # Dependent on helm_and_spark_operator job

    steps:
      - name: Apply PostgreSQL and Hive Metastore Kubernetes manifests
        run: |
          kubectl apply -f spark8s/postgres_hive_ms/postgres_pvc_hive_ms.yaml -n spark-apps
          kubectl apply -f spark8s/postgres_hive_ms/postgres_secret_hive_ms.yaml -n spark-apps
          kubectl apply -f spark8s/postgres_hive_ms/postgres_stateful_hive_ms.yaml -n spark-apps

      - name: Apply Hive Metastore Configurations
        run: |
          kubectl apply -f spark8s/hive-metastore/hive-metastore-cgf.yaml -n spark-apps
          kubectl apply -f spark8s/hive-metastore/hive-metastore.yaml -n spark-apps
          kubectl apply -f spark8s/hive-metastore/hive-metastore-svc.yaml -n spark-apps
          kubectl apply -f spark8s/hive-metastore/hive-schema.yaml -n spark-apps

  # Job for Trino Deployment
  deploy_trino:
    runs-on: ubuntu-latest
    needs: deploy_postgres_and_hive  # Dependent on deploy_postgres_and_hive job

    steps:
      - name: Add Trino Helm repository
        run: |
          helm repo add trino https://trinodb.github.io/charts
          helm repo update

      - name: Install Trino
        run: |
          helm upgrade --install --values trino/values.yaml trino trino/trino -n trino

  # Job for Deploying Airflow PostgreSQL and Airflow Setup
  deploy_airflow:
    runs-on: ubuntu-latest
    needs: deploy_trino  # Dependent on deploy_trino job

    steps:
      - name: Deploy Airflow PostgreSQL
        run: |
          kubectl apply -f airflow_k8s/postgres_airflow/postgres_secret_airflow.yaml -n airflow
          kubectl apply -f airflow_k8s/postgres_airflow/postgres_pvc_airflow.yaml -n airflow
          kubectl apply -f airflow_k8s/postgres_airflow/postgres_stateful_airflow.yaml -n airflow

      - name: Create Airflow secrets
        run: |
          kubectl create secret generic airflow-postgres \
            --from-literal=connection=postgresql://airflow:airflowpass@postgres-airflow:5432/airflow_metastore \
            -n airflow || echo "Secret airflow-postgres already exists"
          kubectl create secret generic osds-webserver-secret \
            --from-literal="webserver-secret-key=$(python3 -c 'import secrets; print(secrets.token_hex(16))')" \
            -n airflow || echo "Secret osds-webserver-secret already exists"

      - name: Deploy Airflow PVC
        run: |
          kubectl apply -f airflow_k8s/airflow-pvc.yaml -n airflow

      - name: Install Airflow
        run: |
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update
          helm upgrade --install airflow apache-airflow/airflow \
            --namespace airflow \
            --values airflow_k8s/airflow-values.yaml \
            --debug \
            --timeout 10m || echo "Airflow Helm release already exists"

      - name: Create Airflow-Spark ClusterRoleBinding
        run: |
          kubectl create clusterrolebinding default-admin \
            --clusterrole cluster-admin \
            --serviceaccount=airflow:airflow-worker \
            --namespace spark-apps || echo "ClusterRoleBinding already exists"

  # Job for Kafka Setup and DBT ServiceAccount Deployment
  setup_kafka_and_dbt:
    runs-on: ubuntu-latest
    needs: deploy_airflow  # Dependent on deploy_airflow job

    steps:
      - name: Install Strimzi Kafka Operator
        run: |
          helm repo add strimzi https://strimzi.io/charts/
          helm repo update
          helm upgrade --install strimzi strimzi/strimzi-kafka-operator \
            --namespace kafka \
            --wait || echo "Strimzi Helm release already exists"

      - name: Deploy Kafka Persistent and Service
        run: |
          kubectl apply -f kafka/kafka-persistent-single.yaml -n kafka
          kubectl apply -f kafka/kafka-svc.yaml -n kafka

      - name: Create dbt ServiceAccount and ClusterRoleBinding
        run: |
          kubectl create serviceaccount dbt -n dbt || echo "ServiceAccount dbt already exists"
          kubectl create clusterrolebinding dbt-role-binding \
            --clusterrole=edit \
            --serviceaccount=dbt:dbt \
            --namespace=dbt || echo "ClusterRoleBinding dbt-role-binding already exists"

  # Job for Superset Deployment
  deploy_superset:
    runs-on: ubuntu-latest
    needs: setup_kafka_and_dbt  # Dependent on setup_kafka_and_dbt job

    steps:
      - name: Add Superset Helm repository
        run: |
          helm repo add superset https://apache.github.io/superset
          helm repo update

      - name: Install Superset
        run: |
          helm upgrade --install --values superset/superset-values.yaml \
            superset superset/superset -n superset --wait || echo "Superset Helm release already exists"

      - name: Delete Hive Schema Pod
        run: |
          kubectl delete -f spark8s/hive-metastore/hive-schema.yaml -n spark-apps
