name: Deploy Milan to GKE

on:
  push:
    branches: [main]
    paths:
      - 'spark8s/**'
      - 'airflow_k8s/**'
      - 'kafka/**'
      - 'superset/**'
      - 'trino/**'
      - '.github/workflows/**'

env:
  CLUSTER_NAME: osd-k8s-cluster
  CLUSTER_ZONE: us-east1
  NAMESPACES: spark-apps airflow spark-operator trino kafka dbt superset

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Google Auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          install_components: 'gke-gcloud-auth-plugin'

      - name: Configure kubectl
        run: |
          gcloud container clusters get-credentials ${{ env.CLUSTER_NAME }} \
            --zone ${{ env.CLUSTER_ZONE }} \
            --project ${{ secrets.GCP_PROJECT_ID }}

      - name: Create Namespaces
        run: |
          for ns in $NAMESPACES; do
            kubectl create namespace $ns --dry-run=client -o yaml | kubectl apply -f -
          done

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      - name: Add Helm Repositories
        run: |
          helm repo add trino https://trinodb.github.io/charts
          helm repo add apache-airflow https://airflow.apache.org
          helm repo add strimzi https://strimzi.io/charts/
          helm repo add superset https://apache.github.io/superset
          helm repo update

      - name: Deploy Spark Infrastructure
        run: |
          kubectl apply -f spark8s/spark_svc_account.yaml -n spark-apps
          helm upgrade --install osds-release spark8s/spark-operator-1.1.27/spark-operator \
            -n spark-operator --create-namespace --wait --timeout 2m

      - name: Deploy Postgres and Hive Metastore
        run: |
          # Deploy Postgres infrastructure
          kubectl apply -f spark8s/postgres_hive_ms/postgres_pvc_hive_ms.yaml -n spark-apps
          kubectl apply -f spark8s/postgres_hive_ms/postgres_secret_hive_ms.yaml -n spark-apps
          kubectl apply -f spark8s/postgres_hive_ms/postgres_stateful_hive_ms.yaml -n spark-apps
            
          # Deploy Hive metastore configurations
          kubectl apply -f spark8s/hive-metastore/hive-metastore-cgf.yaml -n spark-apps
          kubectl apply -f spark8s/hive-metastore/hive-metastore-svc.yaml -n spark-apps
          
          # Apply Hive schema and wait
          kubectl apply -f spark8s/hive-metastore/hive-schema.yaml -n spark-apps
          
          # Additional 1-minute wait as requested
          echo "Additional wait period for schema stabilization..."
          sleep 60
          
          # Deploy Hive metastore after schema is ready
          kubectl apply -f spark8s/hive-metastore/hive-metastore.yaml -n spark-apps
          
          # Deploy Thrift server
          kubectl apply -f spark8s/spark-thriftserver.yaml -n spark-apps

      - name: Install Trino
        run: helm upgrade --install --values trino/values.yaml trino trino/trino -n trino

      - name: Deploy Airflow Infrastructure
        run: |
          kubectl apply -f airflow_k8s/postgres_airflow/ -n airflow
          kubectl apply -f airflow_k8s/airflow-pvc.yaml -n airflow
          
          # Create secrets
          kubectl create secret generic airflow-postgres \
            --from-literal=connection=postgresql://airflow:airflowpass@postgres-airflow:5432/airflow_metastore \
            -n airflow --dry-run=client -o yaml | kubectl apply -f -
          
          kubectl create secret generic osds-webserver-secret \
            --from-literal="webserver-secret-key=$(python3 -c 'import secrets; print(secrets.token_hex(16))')" \
            -n airflow --dry-run=client -o yaml | kubectl apply -f -

      - name: Install Airflow
        run: |
          helm upgrade --install airflow apache-airflow/airflow \
            --namespace airflow \
            --values airflow_k8s/airflow-values.yaml \
            --timeout 10m

      - name: Configure Airflow Permissions
        run: |
          kubectl create clusterrolebinding default-admin \
            --clusterrole cluster-admin \
            --serviceaccount=airflow:airflow-worker \
            --namespace spark-apps --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy Kafka Infrastructure
        run: |
          helm install strimzi strimzi/strimzi-kafka-operator --namespace kafka --version 0.43.0 --wait
          kubectl apply -f kafka/ -n kafka

      - name: Configure dbt
        run: |
          kubectl create serviceaccount dbt -n dbt --dry-run=client -o yaml | kubectl apply -f -
          kubectl create clusterrolebinding dbt-role-binding \
            --clusterrole=edit \
            --serviceaccount=dbt:dbt \
            --namespace=dbt --dry-run=client -o yaml | kubectl apply -f -

      - name: Install Superset
        run: |
          helm upgrade --install superset superset/superset \
            --values superset/superset-values.yaml \
            --namespace superset --wait

      - name: Cleanup
        if: always()
        run: |
          kubectl delete -f spark8s/hive-metastore/hive-schema.yaml -n spark-apps --ignore-not-found